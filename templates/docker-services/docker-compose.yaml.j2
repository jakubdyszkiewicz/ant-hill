---
version: "2"

x-logging:
  &default-logging
  driver: "json-file"
  options:
    tag: "{% raw %}{{ .Name }}{% endraw %}"

services:
  plex:
    image: linuxserver/plex:1.41.3
    container_name: plex
    environment:
      - PUID=1000
      - PGID=1000
      - VERSION=docker
    ports:
      - 32400:32400
      - "32410:32410/udp"
      - "32412:32412/udp"
      - "32413:32413/udp"
      - "32414:32414/udp"
    volumes:
      - /mnt/data/services/plex:/config
      - /mnt/media/movies:/movies
      - /mnt/media/tvshows:/tvshows
      - /mnt/media/music:/music
    logging: *default-logging
    restart: unless-stopped
  syncthing:
    image: linuxserver/syncthing:2.0.12
    container_name: syncthing
    volumes:
      - /mnt/data/services/syncthing:/config
      - /mnt/data/sync:/mnt/data/sync
    restart: unless-stopped
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Warsaw
    ports:
      - 22000:22000/tcp
      - 22000:22000/udp
      - 21027:21027/udp
    expose:
      - 8343
    logging: *default-logging
  syncthing-healthcheck:
    image: jakubdyszkiewicz/syncthing-healthcheck:0.0.1
    container_name: syncthing-healthcheck
    volumes:
      - /home/{{ ansible_user }}/docker-services/syncthing-healthcheck:/config
    restart: unless-stopped
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Warsaw
      - SYNCTHING_API_KEY={{ syncthing_api_key }}
    logging: *default-logging
  nzbget:
    image: linuxserver/nzbget:24.5.20250207
    container_name: nzbget
    volumes:
      - /mnt/data/services/nzbget:/config
      - /home/{{ ansible_user }}/docker-services/data/downloads:/downloads
    restart: always
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Warsaw
    expose:
      - 6789
    logging: *default-logging
  sonarr:
    image: linuxserver/sonarr:4.0.11
    container_name: sonarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=PST
      - UMASK_SET=022 #optional
    volumes:
      - /mnt/data/services/sonarr:/config
      - /mnt/media/tvshows:/mnt/media/tvshows
      - /home/{{ ansible_user }}/docker-services/data/downloads:/downloads
    expose:
      - 8989
    logging: *default-logging
    restart: unless-stopped
  radarr:
    image: linuxserver/radarr:5.18.4
    container_name: radarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=PST
    volumes:
      - /mnt/data/services/radarr:/config
      - /mnt/media/movies:/mnt/media/movies
      - /home/{{ ansible_user }}/docker-services/data/downloads:/downloads
    expose:
      - 7878
    logging: *default-logging
    restart: unless-stopped
  readarr:
    image: linuxserver/readarr:0.4.10-develop
    container_name: readarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=PST
      - UMASK_SET=022 #optional
    volumes:
      - /mnt/data/services/readarr:/config
      - /mnt/media/books:/mnt/media
      - /home/{{ ansible_user }}/docker-services/data/downloads:/downloads
    expose:
      - 8787
    logging: *default-logging
    restart: unless-stopped
  bazarr:
    image: linuxserver/bazarr:1.5.1
    container_name: bazarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Warsaw
      - UMASK_SET=022 #optional
    volumes:
      - /mnt/data/services/bazarr:/config
      - /mnt/media/tvshows:/mnt/media/tvshows # this path has to be the same as in Sonarr
      - /mnt/media/movies:/mnt/media/movies # # this path has to be the same as in Radarr
    expose:
      - 6767
    logging: *default-logging
    restart: unless-stopped
  adguardhome:
    image: adguard/adguardhome:latest
    container_name: adguardhome
    ports: # bind only to specific network interface, 0.0.0.0 has a clash with builtin Ubuntu systemd caching DNS on 127.0.0.1
      - "{{ inventory_hostname }}:53:53/tcp"
      - "{{ inventory_hostname }}:53:53/udp"
      - "3000:3000/tcp" # do not hide it in Kong yet. I need DNS to access all services in local network
    volumes:
      - /mnt/data/services/adguardhome/work:/opt/adguardhome/work
      - /mnt/data/services/adguardhome/conf:/opt/adguardhome/conf
    logging: *default-logging
    restart: unless-stopped
  ddclient:
    image: linuxserver/ddclient:3.10.0 # stale
    container_name: ddclient
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Warsaw
    volumes:
      - /home/{{ ansible_user }}/docker-services/ddclient:/config
    logging: *default-logging
    restart: unless-stopped
  duplicacy:
    image: saspus/duplicacy-web:v1.8.3
    container_name: duplicacy
    hostname: duplicacy-web
    restart: unless-stopped
    expose:
      - 3875
    environment:
      - USR_ID=1000
      - GRP_ID=1000
      - TZ=Europe/Warsaw
    volumes:
      - /mnt/data/services/duplicacy:/config
      - /home/jakub/docker-services/duplicacy/cache:/cache
      - /home/jakub/docker-services/duplicacy/logs:/logs
      - /mnt/data:/data:ro
    logging: *default-logging
  gickup:
    container_name: gickup
    image: buddyspencer/gickup:0.10.36
    restart: unless-stopped
    user: '1000:1000'
    volumes:
      - /home/{{ ansible_user }}/docker-services/gickup/conf.yml:/gickup/conf.yml
      - /mnt/data/gickup:/data
    logging: *default-logging
  rclone:
    container_name: rclone
    image: rclone/rclone:1.69.0
    restart: unless-stopped
    user: '1000:1000'
    volumes:
      - /home/{{ ansible_user }}/docker-services/rclone:/config/rclone
      - /mnt/data/gdrive:/data
    entrypoint: ["/bin/sh", "/config/rclone/loop.sh"]
    logging: *default-logging
  kong:
    image: kong:3.9.0
    user: '1000:1000'
    container_name: kong
    environment:
      - KONG_PREFIX=/tmp/ # the default /usr/local/kong is not writable by non-root user
      - KONG_DATABASE=off
      - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_PROXY_LISTEN=0.0.0.0:8080, 0.0.0.0:8443 ssl
      - KONG_SSL_CERT=/certs/live/{{ ant_domain }}/fullchain.pem
      - KONG_SSL_CERT_KEY=/certs/live/{{ ant_domain }}/privkey.pem
      - KONG_MEM_CACHE_SIZE=32m
      - KONG_NGINX_WORKER_PROCESSES="1"
    ports:
      - 443:8443
    volumes:
      - /home/{{ ansible_user }}/docker-services/kong:/usr/local/kong/declarative
      - /mnt/data/services/swag/etc/letsencrypt:/certs
    restart: unless-stopped
    logging: *default-logging
  swag:
    image: linuxserver/swag:3.1.0
    container_name: swag
    cap_add:
      - NET_ADMIN
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Warsaw
      - URL={{ ant_domain }} 
      - VALIDATION=dns
      - PROPAGATION=60
      - SUBDOMAINS=wildcard
      - DNSPLUGIN=ovh
      - "EMAIL=jakub.dyszkiewicz@gmail.com"
      - STAGING=false
    volumes:
      - /mnt/data/services/swag:/config
    restart: unless-stopped
    logging: *default-logging
  samba:
    image: crazymax/samba:4.19.9
    container_name: samba
    volumes:
      - /home/{{ ansible_user }}/docker-services/samba:/data
      - /mnt/data/nas:/nas
      - /mnt/backup/timemachine:/timemachine
    environment:
      - TZ=Europe/Warsaw
      - SAMBA_LOG_LEVEL=0
    ports:
      - 445:445
    restart: unless-stopped
    logging: *default-logging
  nodeexporter:
    image: prom/node-exporter:v1.8.2
    container_name: nodeexporter
    network_mode: host # required to collect data about host network
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped
    expose:
      - 9100
    logging: *default-logging
  prometheus:
    image: prom/prometheus:v3.1.0
    container_name: prometheus
    user: '1000:1000'
    extra_hosts: # required to collect data from nodeexporter
      - "host.docker.internal:host-gateway"
    volumes:
      - /home/{{ ansible_user }}/docker-services/prometheus:/etc/prometheus
      - /home/{{ ansible_user }}/docker-services/data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=8760h'
      - '--storage.tsdb.retention.size=50GB'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    expose:
      - 9090
    logging: *default-logging
  grafana:
    image: grafana/grafana:11.5.1
    container_name: grafana
    user: '1000:1000'
    volumes:
      - /home/{{ ansible_user }}/docker-services/data/grafana:/var/lib/grafana
      - /home/{{ ansible_user }}/docker-services/grafana:/etc/grafana/provisioning
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true # secured via Kong
      - GF_AUTH_DISABLE_LOGIN_FORM=true # secured via Kong
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Editor
      - GF_USERS_ALLOW_SIGN_UP=false
    expose:
      - 3000
    restart: unless-stopped
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /cgroup:/cgroup:ro
    restart: unless-stopped
    expose:
      - 8080
    logging: *default-logging
  speedtest:
    image: ghcr.io/danopstech/speedtest_exporter:v0.0.5
    container_name: speedtest
    restart: unless-stopped
    expose:
      - 9090
    logging: *default-logging
  promtail:
    image: grafana/promtail:3.3.2
    container_name: promtail
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers
      - /home/{{ ansible_user }}/docker-services/promtail/docker-config.yml:/etc/promtail/docker-config.yml
    command: -config.file=/etc/promtail/docker-config.yml
    logging: *default-logging
  loki:
    image: grafana/loki:3.3.2
    container_name: loki
    expose:
      - 3100
    command: -config.file=/config/config.yml
    volumes:
      - /home/{{ ansible_user }}/docker-services/loki:/config
    logging: *default-logging
  antui:
    image: jakubdyszkiewicz/ant-ui:0.0.1
    container_name: antui
    expose:
      - 80
    logging: *default-logging
  homeassistant:
    container_name: homeassistant
    image: "ghcr.io/home-assistant/home-assistant:2025.7.1"
    volumes:
      - /home/{{ ansible_user }}/docker-services/homeassistant:/config
      - /etc/localtime:/etc/localtime:ro
    restart: unless-stopped
    ports:
      - 8123:8123
      - 21064:21064
    logging: *default-logging
  mdns-repeater:
    image: jdbeeler/mdns-repeater
    container_name: mdns-repeater
    restart: unless-stopped
    network_mode: host
    privileged: true
    environment:
      - EXTERNAL_INTERFACE=enp1s0
      - DOCKER_NETWORK_NAME=docker-services_default
      - USE_MDNS_REPEATER=1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    logging: *default-logging
  mqtt:
    container_name: mqtt
    image: eclipse-mosquitto:2.0.20
    restart: unless-stopped
    volumes:
      - /mnt/data/services/mqtt:/mosquitto
    expose:
      - 1883
      - 9001
    command: "mosquitto -c /mosquitto-no-auth.conf"
  zigbee2mqtt:
    container_name: zigbee2mqtt
    restart: unless-stopped
    image: koenkk/zigbee2mqtt
    volumes:
      - /mnt/data/services/zigbee2mqtt:/app/data
      - /run/udev:/run/udev:ro
    expose:
      - 8080
    environment:
      - TZ=Europe/Warsaw
    devices:
      - /dev/ttyACM0:/dev/ttyACM0
  calendar-fetcher:
    image: alpine:3.21.2
    container_name: calendar-fetcher
    volumes:
      - /home/{{ ansible_user }}/docker-services/calendar-fetcher:/app
      - /mnt/data/calendar:/data
    entrypoint: ["/bin/sh", "/app/loop.sh"]
    restart: unless-stopped
    logging: *default-logging
  spotify-exporter:
    container_name: spotify-exporter
    restart: unless-stopped
    image: jakubdyszkiewicz/spotify-exporter:v0.0.5
    volumes:
      - /home/{{ ansible_user }}/docker-services/spotify-exporter/config.toml:/app/config.toml
      - /mnt/data/services/spotify-exporter/auth-cache:/app/auth-cache
      - /mnt/data/services/spotify-exporter/out:/app/data
    expose:
      - 8888
    environment:
      - TZ=Europe/Warsaw
    logging: *default-logging
  actual:
    container_name: actual
    restart: unless-stopped
    image: actualbudget/actual-server:25.5.0
    volumes:
      - /mnt/data/services/actual:/data
    expose:
      - 5006
    environment:
      - TZ=Europe/Warsaw
    logging: *default-logging
  paperless-redis:
    container_name: paperless-redis
    image: docker.io/library/redis:8
    restart: unless-stopped
    volumes:
      - /mnt/data/services/paperless/redis:/data
  paperless:
    container_name: paperless
    image: ghcr.io/paperless-ngx/paperless-ngx:latest
    restart: unless-stopped
    depends_on:
      - paperless-redis
    volumes:
      - /mnt/data/services/paperless/data:/usr/src/paperless/data
      - /mnt/data/services/paperless/media:/usr/src/paperless/media
      - /mnt/data/services/paperless/export:/usr/src/paperless/export
      - /mnt/data/services/paperless/consume:/usr/src/paperless/consume
    environment:
      - PAPERLESS_TIME_ZONE=Europe/Warsaw
      - PAPERLESS_URL=https://paperless.{{ ant_domain }}
      - "PAPERLESS_OCR_LANGUAGE=pol+eng"
      - PAPERLESS_OCR_LANGUAGES=pol
      - PAPERLESS_REDIS=redis://paperless-redis:6379
  romm:
    image: rommapp/romm:4.4.1
    container_name: romm
    restart: unless-stopped
    environment:
      - DB_HOST=romm-db
      - DB_NAME=romm # Should match MARIADB_DATABASE in mariadb
      - DB_USER=romm-user # Should match MARIADB_USER in mariadb
      - DB_PASSWD={{ romm_db_passwd }}
      - ROMM_AUTH_SECRET_KEY={{ romm_auth_key }}
      - SCREENSCRAPER_USER= # These are the recommended metadata providers
      - SCREENSCRAPER_PASSWORD= # https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#screenscraper
      - RETROACHIEVEMENTS_API_KEY= # https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#retroachievements
      - STEAMGRIDDB_API_KEY= # https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#steamgriddb
      - HASHEOUS_API_ENABLED=true # https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#hasheous
    volumes:
      - /mnt/data/services/romm/resources:/romm/resources # Resources fetched from IGDB (covers, screenshots, etc.)
      - /mnt/data/services/romm/redis-data:/redis-data # Cached data for background tasks
      - /mnt/data/services/romm/library:/romm/library # Your game library. Check https://docs.romm.app/latest/Getting-Started/Folder-Structure/ for more details.
      - /mnt/data/services/romm/assets:/romm/assets # Uploaded saves, states, etc.
      - /mnt/data/services/romm/config:/romm/config # (Optional) Path where config.yml is stored
    depends_on:
      romm-db:
        condition: service_healthy
        restart: true
  romm-db:
    image: mariadb:latest
    container_name: romm-db
    restart: unless-stopped
    environment:
      - MARIADB_ROOT_PASSWORD={{ romm_db_passwd }} # Use a unique, secure password
      - MARIADB_DATABASE=romm
      - MARIADB_USER=romm-user
      - MARIADB_PASSWORD={{ romm_db_passwd }}
    volumes:
      - /mnt/data/services/romm/db:/var/lib/mysql
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      start_period: 30s
      start_interval: 10s
      interval: 10s
      timeout: 5s
      retries: 5